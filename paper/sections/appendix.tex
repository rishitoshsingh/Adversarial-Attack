\crefname{appsec}{Appendix}{Appendices}

\appendixtitleon
\begin{appendices}
    \setcounter{equation}{0}
    \numberwithin{equation}{section}

    \crefalias{section}{appsec}

    \section*{Derivation of learning rules}\label{apendix:derivation_of_learning_rule} 

        Consider a trained three-layered neural network with structure $(L-M-N)$ consisting of $L$ inputs, $M$ hidden neurons, and $N$ output neurons. The output of $m^{th}$ neuron $Y_m$ is computed by activation function of net potential ($V_m$), where $V_m$ and $Y_m$ are defined respectively as
        \begin{align} \label{eqn:Vm}
            V_m =  \sum \limits_{l=1}^{L} w_{lm}x_{l} + w_{0m}x^{0}
        \end{align} and	
        \begin{align}\label{eqn:Ym}	
            Y_m =&\quad f(V_m)
        \end{align}
        Similarly, the respective net potential ($V_n$) and output ($Y_n$) of $n^{th}$ output neuron are given as
        \begin{align}\label{eqn:Vn}
            V_n =\sum \limits_{m=1}^{M} w_{mn}Y_{m} + w_{0n}x^{0} 
        \end{align} and 
        \begin{align} \label{eqn:Yn}
            Y_n =& \quad f(V_n)  
        \end{align}
        Let $Y_n^{Adv}$ be the desired adversary output at $n^{th}$ neuron, then the error, $e_n^Y$ at $n^{th}$ output neuron is calculated through the difference between $Y_n$ and $Y_n^{Adv}$, which is expressed as
        \begin{align} \label{eqn:eny} 
            e_n^Y =& Y_n^D - Y_n
        \end{align} 
        and let $x^{Adv}$ be the desired adversarial image, then the error $e_l^x$ at $l^{th}$ input is calculated through the difference between $x_l$ and $x_l^{Adv}$, which is expressed as
        \begin{align} \label{eqn:enx} 
            e_l^x =& x_l^{Adv} - x_l
        \end{align} 
        The cost function $E$ (MSE) can be calculated by
        \begin{align} \label{eqn:mse}
            E =& \quad \frac{1}{2N} \sum \limits_{n=1}^{N} (e_n^Y)^{2} + \lambda \frac{1}{2L} \sum \limits_{l=1}^{L} (e_l^x)^{2}
        \end{align}
         
        During adversarial image generation, inputs are adaptable instead of network weights. The gradient-decent based backpropagation is used to minimize the cost function. The current input $x^{old}$ is updated to $x^{new}$ using $\Delta x$ as 
        \begin{align} \label{eqn:update_rule}
            x^{old} = x^{new} + \Delta x
        \end{align}
        where $\Delta x$ is promotional to negative gradient of cost function $(\nabla_{x}E)$.
        \begin{align} \label{eqn:delta_x}
            \Delta x &= - \eta \nabla_{x} E \nonumber \\ 
                     &= - \eta \cdot \frac{\partial E}{\partial x}
        \end{align}
        For input $x_{l}$, $- \partial E / \partial x_{l}$ is derived using chain rule of derivation.
        \begin{align} \label{eqn:delta_e_x}
            - \frac{\partial E }{\partial x_{l}} = \frac{1}{N} \sum \limits_{n=0}^{N} \left\{ e_n^Y \cdot f'(V_n) \cdot \frac{\partial V_n}{\partial x_l} \right\} + \frac{\lambda}{L} e_l^x
        \end{align}
        Now, substituting \cref{eqn:delta_e_x} in \cref{eqn:delta_x} yields 
        \begin{align} \label{eqn:delta_x_2}
            \Delta x_l  = \frac{\eta}{N} \left\{ \sum \limits_{n=0}^{N} \delta_{n} w_{mn} \right\} \nabla_{x_l} Y_m + \lambda \frac{\eta}{L} e_l^x
        \end{align}
        where $\delta_{n} = e_n^Y f'(V_n)$

        Now using \cref{eqn:Ym,eqn:Vm}, $\nabla_{x_l} Y_m$ can further be simplified using chain rule of derivation
        \begin{align} \label{eqn:nabla_vn}
            \nabla_{x_l} Y_m &= \sum \limits_{m=0}^{M} \left\{ f'(V_m) \frac{\partial V_m}{\partial x_l} \right\} \nonumber \\
            &= \sum \limits_{m=0}^{M} f'(V_m) w_{lm}
        \end{align}
        
        Now, substituting \cref{eqn:nabla_vn} in \cref{eqn:delta_x_2} yields

        \begin{align} \label{eqn:final_learning_eqn}
            \Delta x= \frac{\eta}{N} \sum \limits_{m=0}^{M} \delta_{m} w_{lm} + \lambda \frac{\eta}{L} e_l^x
        \end{align}
        where,
        \begin{align}
            \delta_{m} = \left\{ \sum \limits_{n=0}^{N} \delta_{n} w_{mn} \right\} f'(V_m) \nonumber
        \end{align}      

\end{appendices}